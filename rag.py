# -*- coding: utf-8 -*-
"""RAG.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11VBEyr1rAzfEkoarItR0CrcaXvnmtb4a
"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install -qU pypdf
# %pip install -U langchain
# %pip install -U langchain-community
# %pip install -U langchain-groq
# %pip install langchain-huggingface
# %pip install langgraph

from google.colab import userdata
import os
api_key=userdata.get('GROQ_API_KEY')
os.environ['GROQ_API_KEY'] = api_key

from langchain_groq import ChatGroq

llm = ChatGroq(model='llama-3.3-70b-versatile')

prompt = '''
       messages: Segundo a Nasa quais seriam os benefícios científicos de ir para Marte?
  '''

resposta = llm.invoke(prompt)

resposta

url = 'https://raw.githubusercontent.com/allanspadini/curso-flash-rag/main/m2m_strategy_and_objectives_development.pdf'

from langchain_community.document_loaders import PyPDFLoader

loader = PyPDFLoader(url)
pages = []
for page in loader.lazy_load():
  pages.append(page)

print(f'{pages[0].metadata}\n')

print(pages[0].page_content)

from langchain_core.vectorstores import InMemoryVectorStore
from langchain_huggingface import HuggingFaceEmbeddings

embed_model = HuggingFaceEmbeddings(model_name="mixedbread-ai/mxbai-embed-large-v1")

vector_store = InMemoryVectorStore.from_documents(pages, embed_model)

docs = vector_store.similarity_search("Objectives Devolopment Process", k=2)

for doc in docs:
    print(f'Page {doc.metadata["page"]}: {doc.page_content[:300]}\n')

retriaver = vector_store.as_retriever()

from langchain_core.prompts import ChatPromptTemplate

template = '''
        You're a helpful assistant that only gives aswers bases on the given context. If the answuer is not int the context, say 'I don't know'.
context: {context}
question: {question}
   '''

prompt = ChatPromptTemplate.from_template(template)

from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough

chain = (
    {'context': retriaver, 'question': RunnablePassthrough()}
    | prompt
    | llm
    | StrOutputParser ()
)

from IPython.display import display, Markdown

response = chain.invoke('What are the Objectives Development Process?')
display(Markdown(response))

from langchain_core.tools import tool

def pega_contexto(query: str) -> str:
  '''Pega o contexto baseado na pesquisa '''
  retriaver = vector_store.as_retriaver()
  resultado = retriaver.invoke(query)
  return resultado

def carrega_pdf(url: str):
  loader = PyPDFLoader(url)
  pages = []
  for page in loader.load():
    pages.append(page)

    vectorstore = InMemoryVectorStore.from_documents(pages,embed_model)
    return vectorstore

vector_store_agriculture = carrega_pdf('https://raw.githubusercontent.com/allanspadini/curso-flash-rag/main/agriculture.pdf')

vector_store_dengue = carrega_pdf('https://raw.githubusercontent.com/allanspadini/curso-flash-rag/main/dengue.pdf')

@tool
def pega_contexto_agriculture(query: str) -> str:
    """Pega o contexto sobre agricultura baseado em uma pesquisa."""
    retriever = vector_store_agriculture.as_retriever()
    resultado = retriever.invoke(query)
    return resultado

@tool
def pega_contexto_dengue(query: str) -> str:
    """Pega o contexto sobre dengue baseado em uma pesquisa."""
    retriever = vector_store_dengue.as_retriever()
    resultado = retriever.invoke(query)
    return resultado

tools = [pega_contexto, pega_contexto_agriculture, pega_contexto_dengue]

pega_contexto_dengue.invoke("Cases of dengue we had since the beginning of 2025?")

pega_contexto_dengue.invoke('Case of dengue we had since the beginning of 2025')

from langgraph.prebuilt import create_react_agent

system_prompt = """You're a helpful assistant that only gives answers based on the given context. If the answer is not in the context, say "I don't know"
- pega_contexto: Tool that returns the context based on the users query if the query is about NASA and space travels.
- pega_contexto_agriculture: Tool that returns the context based on the users query if the query is about agriculture.
- pega_contexto_dengue: Tool that returns the context based on the users query if the query is about dengue.
"""

from langgraph.prebuilt import create_react_agent

agente_pdf = create_react_agent(model=llm, tools=tools, prompt=system_prompt)

resultado = agente_pdf.invoke({"messages": [("user", "what causes dengue?")]})
resultado

resultado = agente_pdf.invoke({"messages": [("user", "what causes dengue?")]})
resultado